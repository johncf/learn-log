## May 2024

### GPU Optimization Workshop (hosted by [@ChipHuyen](https://github.com/chiphuyen))

[YouTube video](https://www.youtube.com/watch?v=v_q2JTIqE20),
[Google Doc notes](https://docs.google.com/document/d/1TR_5Ax0rPqTj8I2sA7MH-aa4J7TUUt4Ji9272OP8ZJg/edit)
[Slides and more notes](https://github.com/mlops-discord/gpu-optimization-workshop)

#### Crash Course to GPU Optimization (Mark Saroufim, Meta)

- PyTorch uses eager evaluation
  - Pros: Easy to debug (print statements to inspect values would work)
  - Cons: GPU memory bandwidth becomes the bottleneck due to multiple data transfers between CPU and GPU context.
- A 2022 article by Horace that goes into more details: [Making Deep Learning Go Brrrr](https://horace.io/brrr_intro.html).
- [PyTorch profiler](https://pytorch.org/docs/stable/profiler.html) can be used to examine such bottlenecks
  - Also see [PyTorch profiler recipe](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html).
- [`torch.compile`](https://pytorch.org/docs/stable/torch.compiler.html) can be used to fuse operators and generate an OpenAI Triton kernel.
  - Check if using `mode="reduce-overhead"` leads to better performance.
- When using CUDA backend with selected GPUs, matrix multiplication can be sped-up significantly using Tensor Cores, but with precision trade-offs.
  - To enable the use of Tensor Cores in PyTorch, [set `float32_matmul_precision`](https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html) to `high` or `medium` (the default is `highest`).
  - Note that this could cause accuracy regressions; see [this discussion](https://dev-discuss.pytorch.org/t/pytorch-and-tensorfloat32/504).
- Quantization helps not only speed-up compute-bound workflows, but also memory-bandwidth-bound workflows!
  - This means weights-only quantization not only makes the model size-on-disk smaller, it could also make the model faster on GPU.
  - See [PyTorch Architecture Optimization (torchao)](https://github.com/pytorch/ao) for a library of many quantization algorithms.
  - Also see [`torch.ao` module](https://pytorch.org/docs/stable/quantization.html), and [how it is different](https://discuss.pytorch.org/t/is-torch-ao-quantization-being-migrated-to-torchao-quantization/203539).
- A great example of GPU optimization relevant to LLMs: [FlashAttention](https://arxiv.org/abs/2205.14135); and to get better intuition behind "softmax scaling", read [Online Normalizer](https://arxiv.org/abs/1805.02867).
- Learn [OpenAI Triton](https://github.com/triton-lang/triton).
  - [Official documentation](https://triton-lang.org/main/getting-started/tutorials/index.html)
  - [cuda-mode/triton-index](https://github.com/cuda-mode/triton-index) lists a few examples and links to resources.
  - Inspect and learn from Triton kernels generated by `torch.compile`.
  - [Using user-defined Triton kernels with `torch.compile`](https://pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html).
- Learn CUDA.
  - Book recommendation: Programming Massively Parallel Processors: A Hands-on Approach by David B. Kirk and Wen-mei W. Hwu (e-book available)
  - [cuda-mode/resource-stream](https://github.com/cuda-mode/resource-stream) lists many links to resources.
  - CUDA kernels can be loaded directly from source using `torch.utils.cpp_extension.load_inline` (see [lecture](https://www.youtube.com/watch?v=LuhJEEJQgUM) & [notes](https://github.com/cuda-mode/lectures/blob/main/lecture_001/load_inline.py)).
  - Consider joining [CUDA Mode Discord server](https://discord.gg/cudamode).
- More resources:
  - [Accelerating generative AI](https://pytorch.org/blog/accelerating-generative-ai) (and [part 2](https://pytorch.org/blog/accelerating-generative-ai-2))
  - [ThunderKittens](https://github.com/HazyResearch/ThunderKittens)
  - [Google Colab](https://colab.research.google.com/)
  - [Amazon SageMaker Studio Lab](https://studiolab.sagemaker.aws/)

#### High Performance LLM Serving on NVIDIA GPUs (Sharan Chetlur, NVIDIA)

- [Nvidia TensorRT Model Optimizer](https://github.com/NVIDIA/TensorRT-Model-Optimizer) is a library of model optimization techniques such as quantization. Mainly aimed at LLMs and diffusion models.
- The talk discusses memory-management inefficiencies with LLM inference, and ideas on how it could be improved.
  - [Paper on PagedAttention](https://arxiv.org/abs/2309.06180)
- Also quickly mentions speculative decoding.
  - [Original paper on speculative decoding](https://arxiv.org/abs/2302.01318)
  - [Recent review paper on speculative decoding](https://arxiv.org/abs/2402.01528v1)

#### Block-based GPU Programming with Triton (Philippe Tillet, OpenAI)

- Triton is an abstraction that is higher level (and simpler) than low-level compute libraries (such as CUDA), but is more expressive (but more complicated) than graph compilers (such as PyTorch).
- Discusses the Machine Model and Programming Model.
- Although Triton code is written in Python, `triton.jit` analyzes the syntax tree and generates code based on it.
- Shows how to implement vector addition and softmax in Triton.
- `torch.compile` should produce good Triton kernels by default.
- Triton has experimental support for AMD GPUs too. ([Link to issue](https://github.com/triton-lang/triton/issues/46))
- Explains the kinds of optimizations that you get when using Triton.

#### Scaling from CPUs to distributed GPUs (William Malpica, Voltron Data)

- CPUs are good for row-wise processing, as often found in Online Transactional Processing (OLTP).
- GPUs are good for column-wise processing, as often found in Online Analytical Processing (OLAP).
- Apache Arrow is the most popular format for columnar data storage and transfer.
- Mentions many GPU-accelerated libraries out there:
  - [cupy](https://github.com/cupy/cupy) is like `numpy` + `scipy`.
  - [cudf](https://github.com/rapidsai/cudf) is like `pandas`.
  - [cuml](https://github.com/rapidsai/cuml) is like `scikit-learn`.
  - [cugraph](https://github.com/rapidsai/cugraph) is like `networkx`.
  - [arrayfire](https://github.com/arrayfire/arrayfire) is a general purpose GPU library that has overlaps with above mentioned libraries.
  - [bend](https://github.com/HigherOrderCO/Bend) is a GPU-friendly programming language.
- GPUs are not universally good. GPUs may not make sense when:
  - Data-processing is latency-bound or I/O-bound.
  - Throughput is not very important.
  - The amount of data is not large enough.
  - The processing pipeline needs to make too many switches between CPU and GPU contexts, bottlenecking memory-bandwidth.
- Moving to distributed setup (CPU or GPU), quickly becomes network bound.
  - Infiniband or RoCEv2 can allow distributed GPUs with direct memory access over a network (RDMA), bypassing CPU contexts.
  - Tools that can use GPU-RDMA: Dask+OpenUCX, Spark Rapids, etc.
