## May 2024

### GPU Optimization Workshop (hosted by [@ChipHuyen](https://github.com/chiphuyen))

Link: <https://www.youtube.com/watch?v=v_q2JTIqE20>

#### Crash Course by Mark Saroufim

- PyTorch uses eager evaluation
  - Pros: Easy to debug (print statements to inspect values would work)
  - Cons: GPU memory bandwidth becomes the bottleneck due to multiple data transfers between CPU and GPU context.
- A 2022 article by Horace that goes into more details: [Making Deep Learning Go Brrrr](https://horace.io/brrr_intro.html).
- [PyTorch profiler](https://pytorch.org/docs/stable/profiler.html) can be used to examine such bottlenecks
  - Also see [PyTorch profiler recipe](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html).
- [`torch.compile`](https://pytorch.org/docs/stable/torch.compiler.html) can be used to fuse operators and generate an OpenAI Triton kernel.
  - Check if using `mode="reduce-overhead"` leads to better performance.
- When using CUDA backend with selected GPUs, matrix multiplication can be sped-up significantly using Tensor Cores, but with precision trade-offs.
  - To enable the use of Tensor Cores in PyTorch, [set `float32_matmul_precision`](https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html) to `high` or `medium` (the default is `highest`).
  - Note that this could cause accuracy regressions; see [this discussion](https://dev-discuss.pytorch.org/t/pytorch-and-tensorfloat32/504).
- Quantization helps not only speed-up compute-bound workflows, but also memory-bandwidth-bound workflows!
  - This means weights-only quantization not only makes the model size-on-disk smaller, it could also make the model faster on GPU.
  - See [PyTorch Architecture Optimization (torchao)](https://github.com/pytorch/ao) for a library of many quantization algorithms.
- A great example of GPU optimization relevant to LLMs: [FlashAttention](https://arxiv.org/abs/2205.14135); and to get better intuition behind "softmax scaling", read [Online Normalizer](https://arxiv.org/abs/1805.02867).
- Learn [OpenAI Triton](https://github.com/triton-lang/triton).
  - Inspect and learn from Triton kernels generated by `torch.compile`
  - [cuda-mode/triton-index](https://github.com/cuda-mode/triton-index) lists a few examples and links to resources.
- Learn CUDA.
  - Book recommendation: Programming Massively Parallel Processors: A Hands-on Approach by David B. Kirk and Wen-mei W. Hwu (e-book available)
  - [cuda-mode/resource-stream](https://github.com/cuda-mode/resource-stream) lists many links to resources.
  - CUDA kernels can be loaded directly from source using `torch.utils.cpp_extension.load_inline` (see [lecture](https://www.youtube.com/watch?v=LuhJEEJQgUM) & [notes](https://github.com/cuda-mode/lectures/blob/main/lecture_001/load_inline.py)).
  - Consider joining [CUDA Mode Discord server](https://discord.gg/cudamode).
- More resources:
  - [Accelerating generative AI](https://pytorch.org/blog/accelerating-generative-ai) (and [part 2](https://pytorch.org/blog/accelerating-generative-ai-2))
  - [ThunderKittens](https://github.com/HazyResearch/ThunderKittens)
  - [Google Colab](https://colab.research.google.com/)
