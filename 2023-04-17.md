## 2023-04-17 - 2023-04-23

### Paper: G. E. Hinton et. al.: Knowledge Distillation (2015)

Link: <https://arxiv.org/abs/1503.02531>

Goal: Transfer knowledge from a larger model (say an ensemble) to a smaller model, effectively and efficiently.

Core idea: A "student" model learns from the "soft" labels generated by a larger "teacher" model, instead of the groundtruth ("hard") labels.

- Having the student model see a richer view of the similarity structure through soft labels seems to help transfer inductive biases (think: generalizing assumptions) from the teacher to the student.
- Useful for compressing knowledge from an ensemble model to a single model
- The student model was observed to learn and generalize well with a fraction of training data (e.g. 3% of original train set) with soft-labels
- The main limitation of this would be the computational cost of training a large model, generating soft-labels, and the storage cost of storing those soft-labels

Further reading:
- S. Abnar et. al. follow-up of Knowledge Distillation <https://arxiv.org/abs/2006.00555>

### Paper: A. Dosovitskiy et. al.: Vision Transformers (ViT)

Link: <https://arxiv.org/abs/2010.11929>

- Input image is split into a number of small patches of fixed size
- Each patch is projected into D dimensension using a trainable linear transformation
- Alternatively, the image could be passed through CNN layers first and patches could be formed from CNN feature maps
- The patch embeddings (+position embeddings) are fed into the transformer encoder, prepended by a [CLS] token embedding (similar to BERT, learnable)
- Model is pre-trained only on classification task (unlike BERT, with 2 tasks)
- TODO Experiments section needs a better look
- Can the model handle images of different aspect ratios?

Further reading:
- H. Touvron et. al. Distilled Vision Transformers <https://arxiv.org/abs/2012.12877>
- M. Dehghani et. al. Scaling to 22B parameters <https://arxiv.org/abs/2302.05442>
