## 2023-07-10 - 2023-07-15

### Lessons learnt from rewriting an AAE to PyTorch

Project: <https://github.com/johncf/mnist-style>

#### Regularization

- Using one or more regularizers greatly boosts training stability.
- One of the most basic regularization, L2-regularization, is equivalent to having a "weight-decay" when using stochastic gradient descent (SGD) without momentum.
- However, when using any momentum-based optimizer (SGD w/ Momentum, RMSProp, Adam etc.), weight-decay is not equivalent to L2-regularization ([source](https://arxiv.org/abs/1711.05101)).
  - In PyTorch, setting [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)'s `weight_decay` parameter (zero by default), seems to implement L2-regularization (and not exactly "weight-decay").
  - In contrast, [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) is the correct implementation of weight-decay as described in the paper.
  - There's [a review paper](https://arxiv.org/abs/2202.00089) on AdamW investigating its merits over Adam+L2.

#### Computational Graphs in PyTorch

- When we do a forward-pass on a model (i.e., from input to output), a computational graph is constructed for backpropagation (by default), containing nodes representing partial derivatives for every operation in the forward pass. ([source](https://pytorch.org/blog/overview-of-pytorch-autograd-engine/))
- Calling [`Tensor.backward()`](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html) (usually on the loss tensor) does backpropagation, calculating (and accumulating) gradients using the above graph to all nodes reachable from it.
- However, we may want to avoid backpropagating gradients past a certain layer (or through a certain tensor) when training different parts of the model. For this, PyTorch provides [`Tensor.detach()`](https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html) to detach a tensor from the compuatational graph.
  - For example, when training the discriminator part of a Generative Adversarial Network, we don't want gradients from the discriminator loss to be backpropagated to the generator part. To accomplish this, when optimizing the discriminator, during its forward pass, instead of feeding the output tensor from the generator as is (which is linked to the compuational graph of generator's forward pass), we must "detach" it first. ([example](https://github.com/johncf/mnist-style/blob/1fc3c9b/mnist_style/tool/train_aae.py#L134))
