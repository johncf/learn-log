## 2023-05-01 - 2023-05-06

### Confidence Estimation

- Estimate the probability that the model's prediction is indeed correct.
- Straightforward in the case of classification problems where each class gets a number associated with it.
  - A softmax on the logits can map the values into a sensible range, but are they representative of the underlying probability of model correctness? Most often not.
- We may assess the correctness of our confidence estimates using a calibration plot (a.k.a reliability diagram).
- We may improve the correctness of our confidence estimates using various calibration techniques such as Platt scaling and Isotonic regression.

Further reading:
- [C. Guo et. al. - On Calibration of Modern Neural Networks](https://arxiv.org/abs/1706.04599)
- [F. KÃ¼ppers et. al. - Confidence Calibration for Object Detection and Segmentation](https://arxiv.org/abs/2202.12785)
- [G. Pereyra et. al. - Regularizing Neural Networks by Penalizing [High Confidence!]](https://arxiv.org/abs/1701.06548)
- [An overview article on confidence calibration](https://dasha.ai/en-us/blog/confidence-calibration-problem-in-machine-learning)

### MLOps Tools

These are tools used for:

- Model registry: a centralized model store, with versioning capabilities
- Data/feature registry: a centralized store for training/validation data (and perhaps feature engineering)
- Experiment tracking and logging: record parameters, metrics, models and results
- Workflow orchestration: to facilitate continuous integration and deployment

A good set of tools should facilitate:
- **Diagnostics**: result analysis, model comparison and debugging during training and validation
- **Versioning** and **reproducibility**: efficiently version and package source code, (frozen) dependencies, hyperparameters and training data associated with each trained model
- Easy **collaboration** between research, engineering and data teams
- **Deployment** and **management** of models using various scalable serving infrastructure
- **Logging** and **monitoring** resource usage, various properties of input/output, and errors, to detect bugs, data drift etc.

Example tools: MLFlow, Kubeflow, Apache Airflow (workflow orchestration tool), AWS SageMaker, etc.

Related:

- [A 2018 article about difficulties of reproducibility](https://petewarden.com/2018/03/19/the-machine-learning-reproducibility-crisis/)
- [An article comparing MLFlow+Airflow with Kubeflow](https://aicurious.io/blog/2022-03-26-airflow-mlflow-or-kubeflow-for-mlops)
- [Data versioning using DVC](https://dvc.org/doc/start/data-management/data-versioning)
