## 2023-05-01 - 2023-05-06

### Feature Selection

Idea: some features may be irrelevant or redundant, and thus be removed with minimal information loss

Techniques

- Filter methods (model-less selection, deterministic in nature)
  - Low variance: ignore features with same value in all samples
  - Missing values: ignore features with significant portion of values missing
  - Collinear features: remove features that are highly (anti-)correlated with other features; i.e. tries to minimize redundancies
    - Fast algorithm (less than quad time complexity): [Fast Correlation-Based Filter (FCBF)](https://www.public.asu.edu/~huanliu/papers/icml03.pdf)
  - Univariate feature selection: remove features that have the lowest predictive value for the target (using statistical tests); considers each feature individually, so redundant features may show similarly high "importance"
- Model-based methods (stochastic in nature, and depends on training initialization and the kind of model used) 
  - Recursive feature elimination
    - Given an estimator that assign weights to features (like coefficients of a linear model), recursively remove the least important feature and repeat until an optimum number of features are obtained
  - L1-based feature selection ([more details](https://scikit-learn.org/stable/modules/feature_selection.html#l1-based-feature-selection))
    - A linear model whose coefficients are penalized with L1-norm is fit, which results in sparse solutions (only a few features gets used)
  - Tree-based feature selection ([more details](https://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation)):
    - Train a tree-based model like random forest or gradient boosted trees
    - For a given decision tree, it is pretty straightforward to compute an "importance score" for each attribute, based on how well it was able to split the train data for prediction (decrease in impurity/entropy).
    - Limitation: derived from the train set, so if the model overfits, "important" features may not be useful for generalization
  - Permutation importance ([more details](https://scikit-learn.org/stable/modules/permutation_importance.html)):
    - Train a model (any model is fine), and compute the performance metric on a dataset (preferrably the test set)
    - Permute the values of a feature in the dataset, and compute the metric again.
    - Repeat the above step several times for each feature.
    - The feature importance is the average reduction in performance metric due to permutation.
    - Limitation: may consider a pair of correlated features to each have low importance even though removing both features may have a huge negative impact.

Related:
- [scikit-learn docs: Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html)
- [Wikipedia: Feature Selection](https://en.wikipedia.org/wiki/Feature_selection)
- [An article in the wild](https://towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0)
