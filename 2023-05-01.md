## 2023-05-01 - 2023-05-06

### Feature Selection

Idea: some features may be irrelevant or redundant, and thus be removed with minimal information loss

Techniques

- Filter methods (model-free selection, deterministic in nature)
  - Low variance: ignore features with same value in all samples
  - Missing values: ignore features with significant portion of values missing
  - Collinear features: remove features that are highly (anti-)correlated with other features; i.e. tries to minimize redundancies
    - Fast algorithm (less than quad time complexity): [Fast Correlation-Based Filter (FCBF)](https://www.public.asu.edu/~huanliu/papers/icml03.pdf)
  - Univariate feature selection: remove features that have the lowest predictive value for the target (using statistical tests); considers each feature individually, so redundant features may show similarly high "importance"
- Model-based methods (stochastic in nature, and depends on training initialization and the kind of model used) 
  - Recursive feature elimination
    - Given an estimator that assign weights to features (like coefficients of a linear model), recursively remove the least important feature and repeat until an optimum number of features are obtained
  - L1-based feature selection ([more details](https://scikit-learn.org/stable/modules/feature_selection.html#l1-based-feature-selection))
    - A linear model whose coefficients are penalized with L1-norm is fit, which results in sparse solutions (only a few features gets used)
  - Tree-based feature selection ([more details](https://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation)):
    - Train a tree-based model like random forest or gradient boosted trees
    - For a given decision tree, it is pretty straightforward to compute an "importance score" for each attribute, based on how well it was able to split the train data for prediction (decrease in impurity/entropy).
    - Limitation: derived from the train set, so if the model overfits, "important" features may not be useful for generalization
  - Permutation importance ([more details](https://scikit-learn.org/stable/modules/permutation_importance.html)):
    - Train a model (any model is fine), and compute the performance metric on a dataset (preferrably the test set)
    - Permute the values of a feature in the dataset, and compute the metric again.
    - Repeat the above step several times for each feature.
    - The feature importance is the average reduction in performance metric due to permutation.
    - Limitation: may consider a pair of correlated features to each have low importance even though removing both features may have a huge negative impact.

Related:
- [scikit-learn docs: Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html)
- [Wikipedia: Feature Selection](https://en.wikipedia.org/wiki/Feature_selection)
- [An article in the wild](https://towardsdatascience.com/a-feature-selection-tool-for-machine-learning-in-python-b64dd23710f0)

### Feature Extraction

Idea: transform arbitrary data (tabular, text or images) into a set of numerical features that are (much) easier to use in learning algorithms

Techniques

- One-hot encoding, to convert a categorical feature into numerical features
- Principal component analysis (PCA), a linear transformation of vector space where dimensions gets ordered by variance; thus the first few dimensions will capture most of the information
- t-distributed Stochastic Neighbor Embedding (t-SNE), non-linear projection to a lower dimensional space where pairwise distribution of distances in both vector spaces are made to be similar (using gradient descent)
- Text feature extraction, to convert text tokens to vectors. Examples: bag of words repr, word2vec, etc.
- Autoencoder, to convert complex data (such as images or text) into a set of vectors

Related:
- [scikit-learn docs: Feature Extraction](https://scikit-learn.org/stable/modules/feature_extraction.html)
- [Wikipedia: Feature Extraction](https://en.wikipedia.org/wiki/Feature_extraction)
- [An article in the wild](https://towardsdatascience.com/feature-extraction-techniques-d619b56e31be)

### Data Visualization

A few broad (and somewhat overlapping) types of visualizations are:
- Categorical distribution visualization
  - Compare distributions of a continuous feature (Y-axis) against values of a categorical feature (X-axis)
  - Does mean/median/variance/etc. differ between subgroups?
  - Use [categorical plots](https://seaborn.pydata.org/generated/seaborn.catplot.html) like violin-plot or box-plot
- Categorical composition visualization
  - Show the categorical makeup of data against a numerical or another categorical feature.
  - How much does each subgroup contribute to the total? How does it change over time?
  - Use pie chart, stacked bar chart, [stacked area plot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.stackplot.html) etc.
- Univariate distribution visualization
  - How spread and/or skewed are the values of a feature?
  - Use [histogram](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.hist.html#pandas.DataFrame.plot.hist) or [KDE plot](https://seaborn.pydata.org/generated/seaborn.kdeplot.html)
- Correlation visualization
  - Illustrate the correlation between features pair-wise
  - Use [bivariate plots](https://seaborn.pydata.org/generated/seaborn.jointplot.html), [pair-wise correlation heatmap](https://stackoverflow.com/a/66506646/2849934), [pair-wise scatter plots](https://seaborn.pydata.org/generated/seaborn.pairplot.html) or [scatter matrix](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.plotting.scatter_matrix.html)
- High-dimensional structure visualization
  - Are there easily derivable structure in the dataset
  - Use [2D or 3D scatter plot of PCA](https://plotly.com/python/pca-visualization/), [Andrews Curves](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.plotting.andrews_curves.html), [parallel coordinates](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.plotting.parallel_coordinates.html), [RadViz](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.plotting.radviz.html)

Related:
- [An article in the wild](https://towardsdatascience.com/the-art-of-effective-visualization-of-multi-dimensional-data-6c7202990c57)
- [Visualizing with polar coordinates](https://matplotlib.org/stable/gallery/pie_and_polar_charts/polar_demo.html)
